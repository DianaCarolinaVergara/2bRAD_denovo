2bRAD de novo walkthrough
November 2, 2017

BEFORE STARTING, replace, in this whole file:
	- yourUserName with your user name
	- matz@utexas.edu by your actual email;
	- cmonstr with your TACC user name.

=============================================

INSTALLATIONS (you can skip these until needed):

------ vcftools: 

git clone https://github.com/vcftools/vcftools.git 
./autogen.sh
./configure --prefix=$HOME/bin/vcftools
make
make install

# adding the program directory to $PATH
cd
nano .bash_profile
	export PATH=$HOME/bin/vcftools/bin:PATH
	# press ctl-O, Enter, ctl-X

re-login to make PATH changes take effect

------- moments: 

git clone https://bitbucket.org/simongravel/moments.git 
cd moments
python setup.py build_ext --inplace

------- ANGSD: 

# install xz first from https://tukaani.org/xz/
cd
wget https://tukaani.org/xz/xz-5.2.3.tar.gz
tar vxf xz-5.2.3.tar.gz 
cd xz-5.2.3/
./configure --prefix=$HOME/xz-5.2.3/
make
make install

# edit .bashrc:
nano .bashrc
   export LD_LIBRARY_PATH=$HOME/xz-5.2.3/lib:$LD_LIBRARY_PATH
   export LIBRARY_PATH=$HOME/xz-5.2.3/lib:$LIBRARY_PATH
   export C_INCLUDE_PATH=$HOME/xz-5.2.3/include:$C_INCLUDE_PATH
logout
# re-login

# now, install htslib:
cd
git clone https://github.com/samtools/htslib.git
cd htslib
make CFLAGS=" -g -Wall -O2 -D_GNU_SOURCE -I$HOME/xz-5.2.3/include"

cd
git clone https://github.com/ANGSD/angsd.git 
cd angsd
make HTSSRC=../htslib

# now adding ANGSD to $PATH
cd
nano .bashrc
# section 2:
   export PATH=$HOME/angsd:$PATH
   export PATH=$HOME/angsd/misc:$PATH
# save (Ctl-O, Ctl-X)

------- fastNGSadmix :

cd
git clone https://github.com/e-jorsboe/fastNGSadmix.git
cd fastNGSadmix 
make
cp fastNGSadmix ~/bin

-------  NGSadmix :
cd ~/bin/
wget popgen.dk/software/download/NGSadmix/ngsadmix32.cpp 
g++ ngsadmix32.cpp -O3 -lpthread -lz -o NGSadmix
cd -

-------  stairwayPlot :

# project page: https://sites.google.com/site/jpopgen/stairway-plot
cdw
wget https://www.dropbox.com/s/tj4i02n36abwjl6/stairway_plot_v0.2.zip?dl=0
mv file stairway_plot_v0.2.zip
unzip stairway_plot_v0.2.zip

-------  ADMIXTURE
cd ~/bin/
wget https://www.genetics.ucla.edu/software/admixture/binaries/admixture_linux-1.23.tar.gz --no-check-certificate
tar vxf admixture_linux-1.23.tar.gz 
mv admixture_linux-1.23/admixture .
cd -

-------  plink 1.9:
cd ~/bin
wget https://www.cog-genomics.org/static/bin/plink171103/plink_linux_x86_64.zip
unzip plink_linux_x86_64.zip
cd -

=============================================
# Setting workspace (loading modules, setting $PATH to where the scripts are going to live)

cd
nano .bashrc

# in Section 1, paste:

  module load launcher
  module load java
  module load cd-hit
  module load fastx_toolkit
  module load samtools
  module load bowtie
  module load picard-tools
  module load gatk
  module load python
  module load Rstats

# in Section 2, paste:
 
   export PATH=$HOME/bin:$PATH

# Ctl-o, Ctl-X

logout
# re-login

==============================================
OK LET'S DO IT!

The idea is to copy the chunks separated by empty lines below and paste them into your cluster 
terminal window consecutively. 

The lines beginning with hash marks (#) are explanations and additional instructions - 
please make sure to read them before copy-pasting. 


# login to cluster
ssh yourUserName@ls5.tacc.utexas.edu

# downloading and installing de-novo 2bRAD scripts in HOME/bin
cd
mkdir bin # make bin directory
cd bin # go there
# cloning github repository
git clone https://github.com/z0on/2bRAD_denovo.git
mv 2bRAD_denovo/* . # move it to here from sub-directory
rm -rf 2bRAD_denovo # remove now-empty directory

# designating all .pl and .py files (perl and pythin scripts) as executable
chmod +x *.pl 
chmod +x *.py

# does it work?
# try running a script from $HOME:
cd
2bRAD_trim_launch.pl
# if you get "command not found" something is wrong! 

#============================
# downloading and preparing the data 

# downloading the data (A.millepora from Orpheus and Keppels)

# making a directory to work in, and going in there:
cds
mkdir RAD
cd RAD

# downloading data from the shared directory
cp /work/01211/cmonstr/lonestar/tjarno/* .

# Trimming the reads
# How does this work? run the trim-launcher script without arguments:
2bRAD_trim_launch.pl

# how do the reads look before trimming (displaying top 25 DNA sequences in O9.fq):
head -50 IB14KIE14_S86_L004_R1_001_trunc.fastq | grep -E "^[ATGCN]+$"

# now for actual trimming and renaming of the files
# creating a file of commands to run:

2bRAD_trim_launch.pl fastq sampleID=1 >trims
# creating a job script:
launcher_creator.py -j trims -n trims -a mega2014 -t 0:10:00 -e matz@utexas.edu
# submitting it to TACC:
sbatch trims.slurm

# what are the commands we are actually running now?
cat trims

# how is our job doing?
squeue -u cmonstr

# Done! do we have the right number of output files?
ll *.fastq | wc -l # this is how many fastq and fq files we started with
ll *.tr0 | wc -l  # this is how many trimmed files we got, should be the same number

# how many reads remained after trimming?
# let's pick one sample, say IB14ESP05:
# how many reads were there originally?
grep @D00 *IB14ESP05*.fastq | wc -l
#how many reads remained?
grep @D00 IB14ESP05.tr0 | wc -l

# DO IT YOURSELF: how do the reads look after trimming? (modify the 'head -50 ...' command above...)

# quality filtering using fastx_toolkit
# creating a list of filtering commands:
ls *.tr0 | perl -pe 's/^(\S+)\.tr0$/cat $1\.tr0 \| fastx_clipper -l 36 -a GGGGGGGGGGGG | fastq_quality_filter -q 20 -p 100 >$1\.trim/' >filt0

# how the filtering commands actually look?
cat filt0
# the options -q 20 -p 100 mean that 100% of all bases within the read should have
# PHRED quality of at least 20 (i.e., probability of error 1% or less) 
# PHRED quality=10*(-log10(P.error))

# DO IT YOURSELF: create a launcher script out of filt0 (5 min time limit) and start the job

# how is our job doing?
squeue -u cmonstr

# Done! do we have the right number of output files (.trim) ?
ll *.trim | wc -l  

# DO IT YOURSELF: how many reads remained after trimming AND filtering in sample O9?

# now the read pre-processing is all done! time for genotyping.

#########################################################
# GENOTYPING

# 'uniquing' individual trimmed fastq files:
ls *.trim | perl -pe 's/^(.+)$/uniquerOne.pl $1 >$1\.uni/' >unii
launcher_creator.py -j unii -n uni1 -a mega2014 -t 0:10:00 -e matz@utexas.edu
sbatch uni1.slurm

# Done! do you have .uni for all your samples?... 
ll *.uni | wc -l  

# how does it look? Let'l display entries from 49,960 to 50,000 :
head -5000 IB14ESP05.trim.uni | tail -40

# merging uniqued files :
#  collecting tags found in at least 5 and at most 20 individuals for tag sharing analysis:
echo "mergeUniq.pl uni minDP=5 minInd=5 maxInd=50 >5to50.uniq" > mer 
launcher_creator.py -j mer -n mjob -a mega2014 -t 0:30:00 -e matz@utexas.edu
sbatch mjob.slurm

# merging nearly all tags (found in at least 2 individuals)  for actual genotyoing
echo "mergeUniq.pl uni minDP=5 minInd=2 >all.uniq" > mer2 
launcher_creator.py -j mer2 -n mjob2 -a mega2014 -q normal -t 0:30:00 -e matz@utexas.edu
sbatch mjob2.slurm

# how does the merged file look? Lets look at entries from 1800 to 2000:
head -2000 all.uniq | tail -200 | less -S
# Q to exit

# how many different kinds of reads we have 
cat all.uniq | wc -l 
# 624816
cat 5to20.uniq | wc -l 
363445

# discarding reads that have more than 7 observations without reverse-complement
head -1 5to50.uniq >5to50.tab
cat 5to50.uniq | awk '!($3>7 && $4==0)' >>5to20.tab

head -1 all.uniq >all.tab
cat all.uniq | awk '!($3>7 && $4==0)' >>all.tab

# re-creating all-fasta file based on filtered tags
awk '{print ">"$1"\n"$2}' all.tab | tail -n +3 > all.fasta

# scp 5to50.uniq file to your laptop for tag-sharing analysis (in R)

#===============================
#  de-novo genotyping

# Which alleles belong to the same loci?
# clustering allowing for up to 3 mismatches (-c 0.91); the most abundant sequence becomes reference
cd-hit-est -i all.fasta -o cdh_alltags.fas -aL 1 -aS 1 -g 1 -c 0.91 -M 0 -T 0

# do we have new huge cdh_alltags files created?
# list the newest 10 files in the directory:
ll -t | head

#------------
# making fake reference genome (of 30 chromosomes) out ot cd-hit cluster representatives

concatFasta.pl fasta=cdh_alltags.fas num=30

# from here on, all the procedures are the same as when mapping to a reference genome

# formatting the fake genome
export GENOME_FASTA=cdh_alltags_cc.fasta
export GENOME_DICT=cdh_alltags_cc.dict 
bowtie2-build $GENOME_FASTA $GENOME_FASTA
samtools faidx $GENOME_FASTA
java -jar $TACC_PICARD_DIR/picard.jar CreateSequenceDictionary R=$GENOME_FASTA  O=$GENOME_DICT

#------------------
# mapping to the (fake) reference genome and creating BAM files

export GENOME_FASTA=cdh_alltags_cc.fasta

# creating list of commands put of file list using perl -pe
ls *trim | perl -pe 's/(\S+)/bowtie2 --no-unal -x \$GENOME_FASTA -U $1 -S $1\.sam/' > bt2
launcher_creator.py -j bt2 -n maps -t 0:30:00 -a mega2014 -e matz@utexas.edu
sbatch maps.slurm

# mapping efficiency?
tail -100 maps.e*

# what does this line do?
ls *.sam > sams

# next stage is compressing, sorting and indexing the sam files (they become bam files)
export GENOME_FASTA=cdh_alltags_cc.fasta
cat sams | perl -pe 's/(\S+)\.sam/samtools import \$GENOME_FASTA $1\.sam $1\.unsorted\.bam && samtools sort -o $1\.sorted\.bam $1\.unsorted\.bam && java -Xmx5g -jar \$TACC_PICARD_DIR\/picard\.jar AddOrReplaceReadGroups INPUT=$1\.sorted\.bam OUTPUT=$1\.bam RGID=group1 RGLB=lib1 RGPL=illumina RGPU=unit1 RGSM=$1 && samtools index $1\.bam/' >s2b
launcher_creator.py -j s2b -n s2b -t 0:05:00 -a mega2014 -e matz@utexas.edu
sbatch s2b.slurm

rm *sorted*
ls *bam | wc -l  # should be the same number as number of trim files

#==========================
# ANDSD => PCA, admixture

# open interactive session
idev

# Generating genotype likelihoods from highly confident SNPs
# -GL 1 : samtools likelihood model
# -doGlf 2 : beagle format
# -doGeno 32 : binary genotype likelihoods format
# -doMajorMinor 1 : infer major and minor alleles from data (not from reference)

FILTERS="-uniqueOnly 1 -remove_bads 1 -minMapQ 30 -minQ 30 -minInd 50 -snp_pval 1e-5 -minMaf 0.05 -dosnpstat 1 -doHWE 1 -hwe_pval 1e-5 -sb_pval 1e-5"
GENOME_FASTA=cdh_alltags_cc.fasta
DOS="-doMajorMinor 1 -doMaf 1 -doCounts 1 -makeMatrix 1 -doIBS 1 -doCov 1 -doGeno 32 -doPost 1 -doGlf 2"
ls *bam >bams
# nano bam file, manually remove extra clones => bams_noclones

angsd -b bams_noclones -GL 1 $FILTERS $DOS -P 40 -out 3pops_noclones

# how many SNPs?
NSITES=`zcat 3pops_noclones.beagle.gz | wc -l`
echo $NSITES
# 18428

# covariance matrix for PCA (if coverage is approximately equal across samples):
gunzip 3pops_noclones.geno.gz
ngsCovar -probfile 3pops_noclones.geno -outfile noclones.covar -nind 61 -nsites $NSITES -call 0 -norm 0 

# if coverage is unequal, use 3pops_noclones.covMat and 3pops_noclones.ibsMat from angsd run

#  ADMIXTURE for K from 2 to 5
for K in `seq 2 5` ; 
do 
NGSadmix -likes 3pops_noclones.beagle.gz -K $K -P 10 -o 3pops_noclones_k${K};
done

scp all the *noclones* files to laptop

#==========================
# ANDSD => SFS for demographic analysis

# enter interactive session
idev

# creating list of SNP sites for SFS, for all pops:
# running ANGSD without snp_pval and with minMaf filter set to ignore singletons (normally, there would be no minMaf filter but here we did not remove PCR duplicates 
# using stronger -minInd filter

FILTERS="-minMapQ 30 -minQ 30 -minInd 50 -minMaf 0.02 -dosnpstat 1 -doHWE 1 -hwe_pval 1e-5 -sb_pval 1e-5"
 DOS="-doMajorMinor 1 -doMaf 1"
angsd -b bams_noclones -GL 1 $FILTERS $DOS -P 48 -out sfsSites

# extracting list of sites to work with
zcat sfsSites.mafs.gz | cut -f 1,2 | tail -n +2 | sort -k 1,1 -k 2,2n | perl -pe 's/\s/:/' >sites2do

# exit interactive session
exit

# how many sites?
cat sites2do | wc -l
33791

# making lists of bam files for each pop
grep KIE bams_noclones >kie
grep RAU bams_noclones >rau
grep ESP bams_noclones >esp

# unfolded (for moments)
echo 'angsd -b kie -rf sites2do -GL 1 -doSaf 1 -anc cdh_alltags_cc.fasta -P 1 -out kie
angsd -b esp -rf sites2do -GL 1 -doSaf 1 -anc cdh_alltags_cc.fasta -P 1 -out esp
angsd -b rau -rf sites2do -GL 1 -doSaf 1 -anc cdh_alltags_cc.fasta -P 1 -out rau' >sfs0
launcher_creator.py -j sfs0 -n sfs0 -t 1:00:00 -a mega2014 -e matz@utexas.edu
sbatch sfs0.slurm

# printing sfs probabilities
realSFS print kie.saf.idx >kie.sfs
realSFS print rau.saf.idx >rau.sfs
realSFS print esp.saf.idx >esp.sfs

# making dadi-SNP counts table based on probabilities
ls *.sfs >sfss
Rscript sfs2dadi.R infiles=sfss  prefix=3pops

#-----------------
# 2d AFS analysis using moments
# install moments (python package)

# get Misha's moments scripts collection
cd
git clone https://github.com/z0on/AFS-analysis-with-moments.git
mv AFS-analysis-with-moments/* ~/bin
rm -rf AFS-analysis-with-moments 

# NB: must mask singletons and private alleles during SFS analysis! 

# print 2d SFS for kie and rau:
python 2dAFS_fold.py 3pops_dadi.data kie rau 40 42

# S2M model for kie and rau (nu1, nu2, T, m12, m21 = params):
python S2M_fold.py 3pops_dadi.data kie rau 40 42 1 1 1 1 1

# IM2 model for kie and rau (nu1_0,nu2_0,nu1,nu2,T,m12,m21 = params):
python IM2_fold.py 3pops_dadi.data kie rau 40 42 1 1 1 1 1 1 1

#-----------------

# stairway plot: historical population sizes for ESP

(ignoring singletons - because of no deduplication, conservative)

realSFS esp.saf.idx >esp_sfs
cat esp_sfs
3759.234804 5082.773410 4901.517283 2725.871576 1952.031534 1325.980506 1176.750821 941.780456 567.845373 817.837276 401.409512 984.552254 16.381838 644.826881 61.597748 565.959387 2.839908 442.083707 94.392939 113.486667 218.516297 82.584333 87.582359 11.942078 0.002080 69.575571 4.658302 0.000226 0.000962 43.606802 0.079898 0.001061 10.795316 0.000000 24.617357 4.025453 47.096118 37.027363 59.100985 69.955557 50.678000 

# copy last 
nano esp.blueprint
#example blueprint file
#input setting
popid: ESP # id of the population (no white space)
nseq: 40 # number of sequences
L: 500000 # total number of observed nucleic sites, including polymorphic and monomorphic
whether_folded: true # whethr the SFS is folded (true or false)
SFS: 5082.773410 4901.517283 2725.871576 1952.031534 1325.980506 1176.750821 941.780456 567.845373 817.837276 401.409512 984.552254 16.381838 644.826881 61.597748 565.959387 2.839908 442.083707 94.392939 113.486667 218.516297 82.584333 87.582359 11.942078 0.002080 69.575571 4.658302 0.000226 0.000962 43.606802 0.079898 0.001061 10.795316 0.000000 24.617357 4.025453 47.096118 37.027363 59.100985 69.955557
#smallest_size_of_SFS_bin_used_for_estimation: 2 # default is 1; to ignore singletons, change this number to 2
#largest_size_of_SFS_bin_used_for_estimation: # default is n-2; to ignore singletons, change this number to nseq-2
pct_training: 0.67 # percentage of sites for training
nrand: 9 18 28 38 # number of random break points for each try (separated by white space)roughly (nseq-2)/4, (nseq-2)/2, (nseq-2)*3/4, nseq-2
project_dir: ESP # project directory
stairway_plot_dir: $WORK/stairway_plot_v2beta/stairway_plot_es # directory to the stairway plot files
ninput: 200 # number of input files to be created for each estimation
#output setting
mu: 2e-8 # assumed mutation rate per site per generation
year_per_generation: 0.25 # assumed generation time (in years)
#plot setting
plot_title: esp # title of the plot
xrange: 0.1,1000 # Time (1k year) range; format: xmin,xmax; "0,0" for default
yrange: 0,0 # Ne (1k individual) range; format: xmin,xmax; "0,0" for default
xspacing: 2 # X axis spacing
yspacing: 2 # Y axis spacing
fontsize: 12 # Font size


java -cp $WORK/stairway_plot_v2beta/stairway_plot_es Stairbuilder esp.blueprint 
grep swarmops esp.blueprint.sh > addTheta
launcher_creator.py -j addTheta -n at -e matz@utexas.edu -a tagmap -t 0:15:00 -q normal
nano at.slurm
#SBATCH -N 18
sbatch at.slurm
grep -v swarmops esp.blueprint.sh >movesPlots
bash movesPlots

scp ESP/*.summary* to laptop

#==========================

#        G  A  T  K

# (use only for high-coverage data, >10x after deduplication)

export GENOME_REF=cdh_alltags_cc.fasta
ls *.bam > bams

# writing command script
echo '#!/bin/bash
#SBATCH -J gt
#SBATCH -n 40
#SBATCH -N 1
#SBATCH -p development
#SBATCH -o gt.o%j
#SBATCH -e gt.e%j
#SBATCH -t 2:00:00
#SBATCH -A mega2014
#SBATCH --mail-type=ALL
#SBATCH --mail-user=matz@utexas.edu
java -jar $TACC_GATK_DIR/GenomeAnalysisTK.jar -T UnifiedGenotyper \
-R $GENOME_REF -nt 40 -nct 1 \
--genotype_likelihoods_model SNP \' >unig2
cat bams | perl -pe 's/(\S+\.bam)/-I $1 \\/' >> unig2
echo '-o primary.vcf ' >> unig2
sbatch unig2

#----------
# Variant quality score recalibration (VQSR)

# making a tab-delimited table of clone (replicate) sample pairs
nano clonepairs.tab
# paste clone pairs, tab delimited, one pair per line
IB14ESP07A.trim	IB14ESP07B.trim
IB14ESP09A.trim	IB14ESP09B.trim
IB14ESP21A.trim	IB14ESP21B.trim
IB14ESP30A.trim	IB14ESP30B.trim
IB14KIE04a.trim	IB14KIE04b.trim
IB14KIE07a.trim	IB14KIE07b.trim
IB14KIE08a.trim	IB14KIE08b.trim
IB14KIE13a.trim	IB14KIE13b.trim
IB14RAU05a.trim	IB14RAU05b.trim
# Ctl-O , enter, Ctl-X

# extracting "true snps" subset (reproducible across replicates) 
# parameter hetPairs can vary depending on replication scheme (3 is good when you have triplicates)
replicatesMatch.pl vcf=primary.vcf replicates=clonepairs.tab hetPairs=2 max.het=0.5 > vqsr.vcf

# determining transition-transversion ratio for true snps (will need it for tranche calibration)
vcftools --vcf vqsr.vcf --TsTv-summary
# Ts/Tv ratio: 1.244  # put your actual number into the next code chunk, --target_titv

# creating recalibration models
export GENOME_REF=cdh_alltags_cc.fasta
java -jar $TACC_GATK_DIR/GenomeAnalysisTK.jar -T VariantRecalibrator \
-R $GENOME_REF -input primary_n.vcf -nt 12 \
-resource:repmatch,known=true,training=true,truth=true,prior=30  vqsr.vcf \
-an QD -an MQ -an FS -mode SNP --maxGaussians 6 \
--target_titv 1.244 -tranche 85.0 -tranche 90.0 -tranche 95.0 -tranche 99.0 -tranche 100 \
-recalFile primary.recal -tranchesFile recalibrate.tranches -rscriptFile recalibrate_plots.R 

# examine output and recalibrate*.pdf files - see which tranche to choose the one before TsTv dropoff
# next chunk assumes we are choosing tranche 95

# applying recalibration (95% tranche)
export GENOME_REF=cdh_alltags_cc.fasta
java -jar $TACC_GATK_DIR/GenomeAnalysisTK.jar -T ApplyRecalibration \
-R $GENOME_REF -input primary_n.vcf -nt 12 \
--ts_filter_level 95.0 -mode SNP \
-recalFile primary.recal -tranchesFile recalibrate.tranches -o recal.vcf

#---------------
# Applying filters

# Tjarno peeps: SKIP THIS BIT
# identifying poorly genotyped individuals
vcftools --vcf recal.vcf --het
# look at number of sites genotyped per individual (4th column): 
cat out.het 
# see if some samples are much lower in the number of sites than others
# for example, if you want to remove samples showing less than 40000 sites:
cat out.het | awk '$4<40000' | cut -f 1  > underSequenced
cat underSequenced

# applying filter and selecting polymorphic biallelic loci genotyped in 90% or more individuals
# (harsh genotyping rate cutoff is strongly recommended for best quality and to avoid RAD loci affected by null 
# alleles because of mutations in restriction site)
vcftools --vcf recal.vcf --remove-filtered-all --max-missing 0.9  --min-alleles 2 --max-alleles 2 --recode-INFO-all --recode --out filt

# selecting only polymorphic sites (they all are in denovo pipeline!) and sites with no excess heterozygosity
grep -E "#|0/1|0/0.+1/1|1/1.+0/0" filt.recode.vcf >polymorphs.vcf
hetfilter.pl vcf=polymorphs.vcf maxhet=0.5 >best.vcf

#---------------
# Final touches

# genotypic match between pairs of replicates (the most important one is the last one, HetsDiscoveryRate)	
repMatchStats.pl vcf=best.vcf replicates=clonepairs.tab 
pair	gtyped	match	[ 00	01	11 ]	HetMatch	HomoHetMismatch	HetNoCall	HetsDiscoveryRate
IB14ESP07A.trim:IB14ESP07B.trim	30816	27298(88.6%)	 [93%	5%	1% ]	1459	1773	167	0.60	
IB14ESP09A.trim:IB14ESP09B.trim	31172	27825(89.3%)	 [93%	6%	1% ]	1623	1766	128	0.63	
IB14ESP21A.trim:IB14ESP21B.trim	30415	26845(88.3%)	 [93%	5%	1% ]	1406	1759	183	0.59	
IB14ESP30A.trim:IB14ESP30B.trim	31056	27528(88.6%)	 [94%	5%	1% ]	1388	1695	161	0.60	
IB14KIE04a.trim:IB14KIE04b.trim	32397	30433(93.9%)	 [91%	8%	1% ]	2542	1431	44	0.78	
IB14KIE07a.trim:IB14KIE07b.trim	32118	30026(93.5%)	 [91%	8%	1% ]	2325	1658	62	0.73	
IB14KIE08a.trim:IB14KIE08b.trim	30406	28597(94.1%)	 [91%	8%	1% ]	2186	1525	234	0.71	
IB14KIE13a.trim:IB14KIE13b.trim	31826	29723(93.4%)	 [91%	8%	1% ]	2281	1649	91	0.72	
IB14RAU05a.trim:IB14RAU05b.trim	32164	29807(92.7%)	 [92%	7%	1% ]	2066	1544	85	0.72	

# looking at per-individual inbreeding 
# positive - excess false homozygotes (due to poor coverage); negative - false heterozygotes (possibly lumped paralogs)
vcftools --vcf best.vcf --het
cat out.het

# create a file listing clones (and low-site/high-homozygosity individuals, if any) to remove
cat clonepairs.tab | cut -f 2 >clones2remove

# removing clones and badly genotyped ones
vcftools --vcf best.vcf --remove clones2remove --recode --recode-INFO-all --out final

# thinning for Fst / PCA / ADMIXTURE  (choosing one SNP per tag with max allele frequency):
thinner.pl vcf=final.recode.vcf criterion=maxAF >thinMaxaf.vcf
10581 loci selected

#----------
# ADMIXTURE

# creating a dataset with fake "chr" chromosome designations
cat thinMaxaf.vcf | perl -pe 's/tag(\d)(\d+)\t(\d+)/chr$1\t$3$2/'>thinMaxChrom.vcf

# reformatting VCF into plink binary BED format
plink --vcf thinMaxChrom.vcf --make-bed --out rads

# ADMIXTURE with cross-validation to select K 
# (bash script to run admixture with several different K's)
for K in 1 2 3 4 5; do admixture --cv rads.bed $K | tee log${K}.out; done

# minimal cross-validation error = optimal K
grep -h CV log*.out

grep '#CHROM' thinMaxChrom.vcf | perl -pe 's/\t/\n/g' | tail -n +10 | perl -pe 's/^IB14(...)(.+)/IB14$1$2\t$1/' > inds2pops

# scp the *.Q and inds2pops to laptop, plot it in R:
tbl=read.table("rads.3.Q")
barplot(t(as.matrix(tbl)), col=rainbow(5),xlab="Individual #", ylab="Ancestry", border=NA)

# or, more fancy, use admixturePlotting_V4.R









