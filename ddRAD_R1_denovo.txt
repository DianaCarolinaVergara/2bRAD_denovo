
#=============================================
               INSTALLATIONS
#=============================================

------- cutadapt: 

cd
pip install --user cutadapt
cp .local/bin/cutadapt ~/bin

------- jellyfish: 

wget https://github.com/gmarcais/Jellyfish/releases/download/v2.3.0/jellyfish-2.3.0.tar.gz
tar vxf jellyfish-2.3.0.tar.gz
cd jellyfish-2.3.0/
./configure --prefix=$HOME
make
make install

------- cd-hit:

git clone https://github.com/weizhongli/cdhit.git
cd cd-hit
make

-------- Misha's 2bRAD scripts

cd ~/bin
git clone https://github.com/z0on/2bRAD_denovo.git
chmod +x 2bRAD_denovo/*.PL
mv 2bRAD_denovo/* .
rm -rf 2bRAD_denovo

#------- ANGSD (note: version numbers might be outdated, replace with latest): 

# install xz first from https://tukaani.org/xz/
cd
wget https://tukaani.org/xz/xz-5.2.3.tar.gz --no-check-certificate
tar vxf xz-5.2.3.tar.gz 
cd xz-5.2.3/
./configure --prefix=$HOME/xz-5.2.3/
make
make install

# edit .bashrc:
nano .bashrc
   export LD_LIBRARY_PATH=$HOME/xz-5.2.3/lib:$LD_LIBRARY_PATH
   export LIBRARY_PATH=$HOME/xz-5.2.3/lib:$LIBRARY_PATH
   export C_INCLUDE_PATH=$HOME/xz-5.2.3/include:$C_INCLUDE_PATH
logout
# re-login

# now, install htslib:
cd
git clone https://github.com/samtools/htslib.git
cd htslib
make CFLAGS=" -g -Wall -O2 -D_GNU_SOURCE -I$HOME/xz-5.2.3/include"

cd
git clone https://github.com/ANGSD/angsd.git 
cd angsd
make HTSSRC=../htslib

# now adding ANGSD to $PATH
cd
nano .bashrc
   export PATH=$HOME/angsd:$PATH
   export PATH=$HOME/angsd/misc:$PATH
# save (Ctl-O, Ctl-X)

-------  ngsTools :

cd ~/bin
git clone https://github.com/mfumagalli/ngsPopGen.git
cd ngsPopGen
make
mv ngs* ..
cd -

-------  NGSadmix :
cd ~/bin/
wget popgen.dk/software/download/NGSadmix/ngsadmix32.cpp 
g++ ngsadmix32.cpp -O3 -lpthread -lz -o NGSadmix
cd -

# assuming $HOME/bin is in your $PATH already; if not, add it (edit .bashrc and re-login)


#=============================================
       ddRAD R1 read processing
#=============================================

# moving R2 reads away; not going to use them
mkdir R2
mv *_R2* R2

# look at reads
head -50 myfile.fastq | grep -E "^[ATGCN]+$"

# de-multiplexing, leaving tag 120b in length:
export TagLen=120
>tdd
for D in `ls *.fastq`;do echo "trim_ddRAD.pl fastq=$D minBCcount=80000 length=$TagLen tagsPerRead=1 sampleID=2">>tdd;done
# execute all commands in tdd

# quality filtering using cutadapt (see installation above)
# removing reads with qualities at ends less than Q15
export TagLen=120
>trimse
for file in *.tr0; do
echo "cutadapt --format fastq -q 15,15 -m $TagLen -o ${file/.tr0/}.trim $file > ${file}_trimlog.txt" >> trimse;
done
# execute all commands in trimse

#------------------ "stacking"  (essentially, kmer analysis with kmer length = read length)

# converting fastq to fasta (for kmer analysis)
>f2f
for F in `ls *trim`; do echo "fastq_to_fasta -i $F -o $F.fasta" >>f2f ;done
# execute all commands in f2f

# analyzing kmers
 
export TagLen=120
>jelly
for F in `ls *fasta`; do echo "jellyfish count -m $TagLen -s 100M -t 1 -C $F -o $F.jf && jellyfish dump $F.jf > $F.kmers.fa">>jelly;done
# execute all commands in jelly

# merging kmer lists and filtering kmers aiming to get major alleles
# settings: 
# minimal total number of kmer occurences (minDP)=10, 
# minimum number of individuals with this kmer (minInd)=10, 
# maximum number of individuals with this kmer (maxInd)=[all individuals]
ls *kmers.fa > all.kf
mergeKmers.pl all.kf minDP=10 minInd=10 > all.ktab

# in the mk.o* file, should see report something like this:
# considering kmers seen at least 10 times:8663509 kmers
# passing minDP:10 minInd:10 maxInd:-1 filters: 562020

#------------------ clustering kmers to identify major alleles (for "fake genome" assembly)

# converting kmers to fasta
awk '{print ">"$1"\n"$2}' all.ktab | tail -n +3 > all.fasta

# clustering kmers into loci using cd-hit
# allowing for up to 3% mismatch (-c 0.97); the most abundant sequence becomes "reference" (major allele at a locus)
cd-hit-est -i all.ktab.fasta -o all.clust -aL 1 -aS 1 -g 1 -c 0.97 -M 0 -T 0

# number of inferred RAD loci:
grep ">"  all.ktab.fasta.clust | wc -l

#------------------------- making and formatting "genome" (20 fake chromosomes made of concatenated "loci")

concatFasta.pl fasta=all.ktab.fasta.clust num=20
mv all.ktab.fasta.clust_cc.fasta all_cc.fasta
mv all.ktab.fasta.clust_cc.tab all_cc.tab
samtools faidx all_cc.fasta
bowtie2-build all_cc.fasta all_cc.fasta

#------------------------- mapping trimmed-filtered reads to "genome"

REF=all_cc.fasta
>maps
for F in `ls *.trim`; do
echo "bowtie2 --no-unal -x $REF -U $F -S $F.sam">>maps
done
# execute all commands in maps

#------------------------- converting sams to bams, indexing

>s2b
for file in *.sam; do
echo "samtools sort -O bam -o ${file/.sam/}.bam $file && samtools index ${file/.sam/}.bam">>s2b;
done

# execute all commands listed in s2b file
ls *bam | wc -l  # should be the same number as number of trim files
ls *bam > bams

#------------------------- quality assessment (feel free to skip), ANGSD (using only chr1 = 1/20th of total)

export GENOME_REF=all_cc.fasta

# angsd settings:
# -minMapQ 20 : only highly unique mappings (prob of erroneous mapping = 1%)
# -baq 1 : realign around indels (not terribly relevant for 2bRAD reads mapped with --local option) 
# -maxDepth : highest total depth (sum over all samples) to assess; set to 10x number of samples

FILTERS="-uniqueOnly 1 -remove_bads 1 -minMapQ 20 -baq 1 -ref $GENOME_REF -maxDepth 1000"

# T O   D O : 
TODO="-doQsDist 1 -doDepth 1 -doCounts 1"

# in the following line, -r argument is one chromosome or contig to work with (no need to do this for whole genome as long as the chosen chromosome or contig is long enough)
# (look up lengths of your contigs in the header of *.sam files)
angsd -b bams -r chr1 -GL 1 $FILTERS $TODO -P 6 -out dd 

# summarizing results (using modified script by Matteo Fumagalli)
Rscript ~/bin/plotQC.R dd  
cat dd.info 
# scp dd.pdf to laptop to look at distribution of base quality scores, fraction of sites in each sample passing coverage thresholds, and fraction of sites passing genotyping rates cutoffs. Use these to guide choices of -minQ,  -minIndDepth and -minInd filters in subsequent ANGSD runs

#--------------- population structure, HWE (common variants, maf>0.05)

# Note: PCA and Admixture are not supposed to be run on data that contain clones (or genotyping replicates); remove them from bams list. If you want to detect clones, however, do keep the replicates and analyse identity-by-state (IBS) matrix (explained below)

# Generating genotype likelihoods from highly confident (non-sequencing-error) SNPs

# F I L T E R S :
# (ALWAYS record your filter settings and explore different combinations to confirm that results are robust. )
# Suggested filters :
# -minMapQ 20 : only highly unique mappings
# -minQ 30 : only highly confident base calls
# -minInd : the site must be genotyped in at least that many individuals (setting this to 80% of  total number of  individuals here)
# -snp_pval 1e-5 : high confidence that the SNP is not just sequencing error 
# -minMaf 0.05 : only common SNPs, with allele frequency 0.05 or more. Consider raising this to 0.1 for population structure analysis.
# Note: the last two filters are very efficient against sequencing errors but introduce bias against true rare alleles. It is OK (and even desirable) - UNLESS we want to do AFS analysis. We will generate data for AFS analysis in the next part.
# also adding filters against very badly non-HWE sites (such as, all calls are heterozygotes => lumped paralog situation):

MI=`cat bams | wc -l | awk '{print int($1*0.8)}'`
FILTERS="-uniqueOnly 1 -remove_bads 1 -minMapQ 20 -minQ 20 -minInd $MI -snp_pval 1e-5 -minMaf 0.05 -dosnpstat 1 -doHWE 1 -hetbias_pval 1e-2 -skipTriallelic 1"

# THINGS FOR ANGSD TO DO : 
# -GL 1 : samtools likelihood model
# -doGlf 2 : output genotype likelihoods in beagle format (for admixture)
# -doPost 1 : output posterior allele frequencies based on HWE prior
# -doMajorMinor 1 : infer major and minor alleles from data (not from reference)
# -makeMatrix 1 -doIBS 1 : identity-by-state and covariance matrices based on single-read resampling (robust to variation in coverage across samples)
TODO="-doMajorMinor 1 -doMaf 1 -doCounts 1 -makeMatrix 1 -doIBS 1 -doGeno 32 -doVcf 1 -doPost 1 -doGlf 2"

# Starting angsd with -P the number of parallel processes. Funny but in many cases angsd runs faster on -P 1
angsd -b bams -GL 1 $FILTERS $TODO -P 1 -out myresult

# how many SNPs?
NSITES=`zcat myresult.beagle.gz | wc -l`
echo $NSITES

# use myresult.covMat and myresult.ibsMat from angsd run for PCoA and PCA (see last line of this section).

# NgsAdmix for K from 2 to 5
for K in `seq 2 5` ; 
do 
NGSadmix -likes myresult.beagle.gz -K $K -P 10 -o mydata_k${K};
done

# scp the *.Q and inds2pops files to laptop, plot it in R:
# use admixturePlotting2a.R to plot (will require minor editing - population names)

# scp *Mat, *covar, *qopt and bams files to laptop, use angsd_ibs_pca.R to plot PCA and admixturePlotting_v4.R to plot ADMIXTURE

#==========================
# ANDSD => SFS and heterozygosity 

# now not filtering for allele frequency
# sb - strand bias filter; only use for 2bRAD, GBS or WGS (not for ddRAD or RADseq)
# hetbias - detects weird heterozygotes because they have unequal representation of alleles 
# setting minInd to 50% of all  individuals (might depend on the results from quality control step)
MI=`cat bams | wc -l | awk '{print int($1*0.5)}'`
GENOME_REF=all_cc.fasta
FILTERS="-uniqueOnly 1 -remove_bads 1  -skipTriallelic 1 -minMapQ 30 -minQ 25 -hetbias_pval 1e-5 -minInd $MI"
TODO="-doHWE 1 -doMajorMinor 1 -doMaf 1 -dosnpstat 1 -doSaf 1 -anc $GENOME_REF -fold 1"
angsd -b bams -GL 1 -P 12 $FILTERS $TODO -out div

# generating SFS
realSFS div.saf.idx -P 6 > div.sfs

# ANGSD website: "For diploid single samples the hetereozygosity is simply the second value in the SFS/AFS" (http://www.popgen.dk/angsd/index.php/Heterozygosity) 
# so, let's make a little R script:
echo 'args = commandArgs(trailingOnly=TRUE)
a=scan(args[1])
print(round(a[2]/sum(a),5))' >het.R
Rscript het.R div.sfs

#-------------- estimate theta 

GENOME_REF=all_cc.fasta
FILTERS="-uniqueOnly 1 -remove_bads 1  -skipTriallelic 1 -minMapQ 30 -minQ 25 -hetbias_pval 1e-5 -minInd $MI"
angsd -bam bams -out div2 -doThetas 1 -doSaf 1 -pest div.sfs -anc $GENOME_REF -GL 1

# log-scale per-site thetas:
thetaStat print div2.thetas.idx 2>/dev/null > logThetas


